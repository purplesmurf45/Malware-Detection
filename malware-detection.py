#!/usr/bin/env python
# coding: utf-8

# In[1]:


# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

import gc
gc.collect();


import warnings
warnings.filterwarnings('ignore')

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All" 
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session


# In[2]:


import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import OneHotEncoder, LabelEncoder, OrdinalEncoder
import sklearn as sklearn
from sklearn.preprocessing import LabelBinarizer

from imblearn.over_sampling import SMOTE
from numpy.random import RandomState
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
# import xgboost as xgb
# from xgboost.sklearn import XGBRegressor
# from sklearn.model_selection import GridSearchCV
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict, StratifiedKFold
import warnings
warnings.filterwarnings('ignore')

get_ipython().run_line_magic('matplotlib', 'inline')
sns.set_palette(['#06B1F0', '#FC4B60'])
random_seed = 63445

from imblearn.ensemble import EasyEnsembleClassifier 


# In[3]:


malware_data = pd.read_csv("/kaggle/input/malware-detection-arjun/train.csv")
df_test = pd.read_csv("/kaggle/input/malware-detection-arjun/test.csv")

df = pd.read_csv("/kaggle/input/malware-detection-arjun/train.csv")

df.set_index('MachineIdentifier', inplace=True)


# In[4]:


df.info()


# In[5]:


to_delete = [col for col in df.columns if df[col].isnull().sum() > 200000]


# In[6]:


to_delete


# In[7]:


df.drop(columns = to_delete, inplace = True)
df_test.drop(columns = to_delete, inplace = True)


# * Fill null values!
# * Remove correlated values.
# * Find out one hot columns.

# ## Imputing Missing Values!

# In[8]:


numerical_attributes = df.select_dtypes(include = ['int', 'float'])
numerical_attributes.isnull().sum()


# In[9]:


df.fillna(df.agg("median"), inplace = True)
df_test.fillna(df.agg("median"), inplace = True)


# In[10]:


df.isnull().sum().sort_values(ascending = False)


# In[11]:



df[["Census_PrimaryDiskTypeName"]].describe()


# In[12]:


df["Census_PrimaryDiskTypeName"].fillna("HDD", inplace = True)
df_test["Census_PrimaryDiskTypeName"].fillna("HDD", inplace = True)


# In[13]:


df[["Census_ChassisTypeName"]].describe()


# In[14]:


df["Census_ChassisTypeName"].fillna("Notebook", inplace = True)
df_test["Census_ChassisTypeName"].fillna("Notebook", inplace = True)


# In[15]:


df[["Census_PowerPlatformRoleName"]].describe()


# In[16]:


df["Census_PowerPlatformRoleName"].fillna("Mobile", inplace = True)
df_test["Census_PowerPlatformRoleName"].fillna("Mobile", inplace = True)


# In[17]:


df.isnull().sum().sum()


# ## Visualising Correlations:

# In[18]:


plt.figure(figsize=(15, 9))
sns.heatmap(numerical_attributes.corr(), vmin=-1, cmap="coolwarm")


# The Census_OSBuildNumber, Census_OSUILocaleIdentifier columns are highly correlated, hence can be dropped.

# In[19]:


df.drop(columns = ["Census_OSBuildNumber", "Census_OSUILocaleIdentifier"], inplace = True)
df_test.drop(columns = ["Census_OSBuildNumber", "Census_OSUILocaleIdentifier"], inplace = True)


# * EasyEnsemble
# * CatBoost
# * SMOTE.

# ## Categorical - columns encoding!

# In[20]:


cat = df.select_dtypes(include = ['object'])
print(len(cat.columns))
for col in cat:
    print('{} : {}'.format(col, df[col].nunique()))


# In[21]:


non_one_hot_cols = cat.apply(lambda col: col.nunique()).loc[lambda x: x > 20].index.tolist()
one_hot_cols = cat.apply(lambda col: col.nunique()).loc[lambda x: x <= 20].index.tolist()


# In[22]:


non_one_hot_cols


# In[23]:


ohe = OneHotEncoder(handle_unknown='ignore', sparse = False)
le = LabelEncoder()
lb = LabelBinarizer()


# In[24]:


df[one_hot_cols]


# In[25]:


for col in non_one_hot_cols:
    le = LabelEncoder().fit(np.unique(df[col].unique().tolist() + df_test[col].unique().tolist()))
    df[col] = le.transform(df[col].values)
    df_test[col] = le.transform(df_test[col].values)


# In[26]:


ohe.fit(df[one_hot_cols])

# the actual feature generation is done in a separate step
tr=ohe.transform(df[one_hot_cols])

# if you need the columns in your existing data frame, you can glue them together
df2=pd.DataFrame(tr, index=df.index)
df_encoded= pd.concat([df, df2], axis='columns')
df_encoded.drop(columns = one_hot_cols, inplace = True)


# In[27]:


tr=ohe.transform(df_test[one_hot_cols])

# if you need the columns in your existing data frame, you can glue them together
df2=pd.DataFrame(tr, index=df_test.index)
df_test_encoded= pd.concat([df_test, df2], axis='columns')
df_test_encoded.drop(columns = one_hot_cols, inplace = True)


# In[28]:


df_encoded.columns


# In[29]:


df_test_encoded.columns


# In[30]:


# Code used for Hyperparameter Tuning:

# def objective(params):
#     params = {
#         'num_leaves': int(params['num_leaves']),
#         'colsample_bytree': '{:.3f}'.format(params['colsample_bytree']),
#     }
    
#     clf = lgbm.LGBMClassifier(
#         n_estimators=500,
#         learning_rate=0.01,
#         **params
#     )
#     sm = SMOTE(random_state=random_seed)
#     X, y = sm.fit_sample(df.drop('HasDetections', axis=1), df['HasDetections'])
    
#     score = cross_val_score(clf, X, y, scoring=make_scorer(roc_auc_score), cv=StratifiedKFold()).mean()
#     print("AUC-ROC {:.3f} params {}".format(score, params))
#     return score

# space = {
#     'num_leaves': hp.quniform('num_leaves', 2**6, 2**12, 2),
#     'colsample_bytree': hp.uniform('colsample_bytree', 0.1, 1.0),
# }

# best = fmin(fn=objective,
#             space=space,
#             algo=tpe.suggest,
#             max_evals=10)

# print("Hyperopt estimated optimum {}".format(best))


# In[31]:


#Random Forest
x_train, x_test, y_train, y_test = train_test_split(df_encoded.drop('HasDetections', axis=1), df_encoded['HasDetections'], test_size = 0.30, random_state = random_seed, stratify = df_encoded['HasDetections'])
# print ("TRAIN DATA SHAPE: ", x_train.shape)
# print ("TEST DATA SHAPE: ", x_test.shape)

# erf = EasyEnsembleClassifier( n_estimators=100, base_estimator= RandomForestClassifier(n_estimators=5, random_state=random_seed))
# erf.fit(x_train, y_train)


# In[32]:


#XGBoost
# xgb_m = xgb.XGBClassifier(objective = 'binary:logistic',
#                          learning_rate = 0.1,
#                          max_depth = 8,
#                          min_child_weight = 1,
#                          subsample = 1,
#                          colsample_bytree = 0.8,
#                          n_estimators = 500,wh
#                          verbosity = 1)
# parameters = {'nthread': [-1],
#               'objective': ['binary:logistic'],
#               'learning_rate': [0.05, 0.10], #so called `eta` value
#               'max_depth': [8],
#               'min_child_weight': [1],
#               'subsample': [1],
#               'colsample_bytree': [0.8],
#               'n_estimators': [100, 500]}
# xgb_grid = GridSearchCV(estimator = xgb_m, param_grid = parameters, cv = 2, scoring = 'f1',  n_jobs = 5, verbose = True)
# xgb_m.fit(x_train, y_train)
# print(xgb_grid.best_score_)
# print(xgb_grid.best_params_)


# In[33]:


# RUSBoost:
# params = {'colsample_bytree': 0.487977817850359, 'num_leaves': 2420}
# x_train, x_test, y_train, y_test = train_test_split(df_encoded.drop('HasDetections', axis=1), df_encoded['HasDetections'], test_size = 0.10, random_state=random_seed, stratify=df_encoded['HasDetections'])
# print ("TRAIN DATA SHAPE: ", x_train.shape)
# print ("TEST DATA SHAPE: ", x_test.shape)
# erf = RUSBoostClassifier( n_estimators=100, base_estimator= lgbm.LGBMClassifier(n_estimators=500,learning_rate=0.01,verbose = 100, **params))
# erf.fit(x_train, y_train)


# In[34]:


#LightGBM
import lightgbm as lgbm

#sm = SMOTE(random_state=random_seed)
#X, y = sm.fit_sample(df_encoded.drop('HasDetections', axis=1), df_encoded['HasDetections'])

X, y = df_encoded.drop('HasDetections', axis=1), df_encoded['HasDetections']

params = {'colsample_bytree': 0.487977817850359, 'num_leaves': 2420}
erf = lgbm.LGBMClassifier(n_estimators=500,learning_rate=0.01,verbose = 100, **params)
erf.fit(X,y)


# In[35]:


from sklearn.metrics import roc_auc_score

def metrics(true, preds):
    """
    Function to calculate evaluation metrics 
    parameters: true values, predictions
    prints accuracy, recall, precision and f1 scores
    """
    accuracy = accuracy_score(true, preds)
    recall = recall_score(true, preds)
    precision = precision_score(true, preds)
    f1score = f1_score(true, preds)
    auc_roc = roc_auc_score(true, preds)
    print('accuracy: {}, recall: {}, precision: {}, f1-score: {} auc-roc: {}'.format(accuracy, recall, precision, f1score, auc_roc))


# In[36]:


preds = erf.predict(x_test)
probs = erf.predict_proba(x_test)

# preds = xgb_m.predict(x_test)
# probs = xgb_m.predict_proba(x_test)

metrics(y_test, preds)


# In[37]:


df_test_encoded["HasDetections"] = pd.Series()

df_test_encoded["HasDetections"] = 1 - erf.predict_proba(df_test_encoded.drop(columns = ["MachineIdentifier", "HasDetections"]))

# df_test_encoded["HasDetections"] = 1 - xgb_m.predict_proba(df_test_encoded.drop(columns = ["MachineIdentifier", "HasDetections"]))


# In[38]:


df_test_encoded[["MachineIdentifier", "HasDetections"]].to_csv('easyensemble_2.csv', index=False)

# df_test_encoded[["MachineIdentifier", "HasDetections"]].to_csv('xgb_grid.csv', index=False)

